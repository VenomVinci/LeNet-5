---

# LeNet-5 Architecture 

This project implements the **exact original LeNet-5 architecture** as proposed by **Yann LeCun et al. (1998)** in the paper *â€œGradient-Based Learning Applied to Document Recognitionâ€*. The model is designed specifically for **handwritten digit recognition (e.g., MNIST dataset)** and is one of the earliest and most influential Convolutional Neural Networks (CNNs).

---

## ğŸ“ Architecture Overview

LeNet-5 follows a **7-layer architecture** (excluding input), consisting of alternating **convolutional and subsampling (pooling) layers**, followed by fully connected layers.

```
Input (32Ã—32)
   â†“
C1 â†’ S2 â†’ C3 â†’ S4 â†’ C5 â†’ F6 â†’ Output
```

Letâ€™s break down **each layer in detail**.

---

## Input Layer

* **Input Size:** `32 Ã— 32 Ã— 1` (grayscale image)
* Although MNIST images are `28 Ã— 28`, they are **padded to 32 Ã— 32** to preserve edge information.

---

## C1 â€“ Convolution Layer 1

* **Type:** Convolution
* **Filters:** 6
* **Kernel Size:** `5 Ã— 5`
* **Stride:** 1
* **Output Size:** `28 Ã— 28 Ã— 6`
* **Parameters:** (5Ã—5Ã—1)Ã—6 + 6 = **156**

### Purpose:

* Detects **basic features** such as edges, corners, and blobs.
* Each filter learns a different low-level feature.

---

## S2 â€“ Subsampling Layer 1 (Average Pooling)

* **Type:** Average Pooling (not max pooling in original LeNet)
* **Kernel Size:** `2 Ã— 2`
* **Stride:** 2
* **Output Size:** `14 Ã— 14 Ã— 6`

### Purpose:

* Reduces spatial dimensions
* Adds **translation invariance**
* Reduces computational cost

>  Note: Original LeNet uses **average pooling with trainable parameters**, not max pooling.

---

## C3 â€“ Convolution Layer 2

* **Filters:** 16
* **Kernel Size:** `5 Ã— 5`
* **Output Size:** `10 Ã— 10 Ã— 16`
* **Connections:** Not fully connected to all S2 maps (partial connection scheme)

### Purpose:

* Learns **more complex patterns** by combining features from C1.
* Detects shapes like curves, combinations of edges, etc.

---

## S4 â€“ Subsampling Layer 2

* **Type:** Average Pooling
* **Kernel Size:** `2 Ã— 2`
* **Stride:** 2
* **Output Size:** `5 Ã— 5 Ã— 16`

### Purpose:

* Further spatial reduction
* Keeps important features while discarding noise

---

## C5 â€“ Convolution Layer 3 (Fully Connected Convolution)

* **Filters:** 120
* **Kernel Size:** `5 Ã— 5`
* **Output Size:** `1 Ã— 1 Ã— 120`

### Why 1Ã—1?

Because input is `5Ã—5`, applying `5Ã—5` filters collapses spatial dimension.

### Purpose:

* Acts like a **fully connected layer**
* Each neuron sees the **entire image**
* Learns high-level representations of digits

---

## F6 â€“ Fully Connected Layer

* **Units:** 84

### Purpose:

* Combines all extracted features
* Learns **digit-specific patterns**
* 84 was chosen intentionally by LeCun (related to ASCII codes)

---

## Output Layer

* **Units:** 10
* **Activation:** Softmax
* **Represents:** Digits **0â€“9**

### Purpose:

* Produces probability distribution over 10 classes

---

## ğŸ“Š Layer-by-Layer Summary Table

| Layer  | Type           | Output Shape | Description            |
| ------ | -------------- | ------------ | ---------------------- |
| Input  | -              | 32Ã—32Ã—1      | Padded grayscale image |
| C1     | Conv (6Ã—5Ã—5)   | 28Ã—28Ã—6      | Low-level features     |
| S2     | Avg Pool       | 14Ã—14Ã—6      | Downsampling           |
| C3     | Conv (16Ã—5Ã—5)  | 10Ã—10Ã—16     | Complex features       |
| S4     | Avg Pool       | 5Ã—5Ã—16       | Downsampling           |
| C5     | Conv (120Ã—5Ã—5) | 1Ã—1Ã—120      | High-level features    |
| F6     | Dense (84)     | 84           | Feature combination    |
| Output | Dense (10)     | 10           | Digit classification   |

---

## Statement for README (Important)

You can **clearly claim** this in your README:

> **â€œThis implementation strictly follows the original LeNet-5 architecture proposed by Yann LeCun et al., without structural modification. All layers, filter sizes, and design principles match the original paper.â€**

Or shorter:

> **â€œThis project implements the exact original LeNet-5 architecture.â€**

---

## ğŸ’¡ Why LeNet is Important

* First successful CNN in real-world application (bank cheque recognition)
* Introduced:

  * Convolution layers
  * Subsampling (pooling)
  * Weight sharing
  * Hierarchical feature learning
* Foundation of **AlexNet, VGG, ResNet, etc.**

---
